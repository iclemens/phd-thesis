\chapter{General introduction}
\newpage

Life began about four billion years ago with the encapsulation of self-replicating RNA in a lipidic membrane \cite<e.g.>{orgel1968}. Simple as these probionts were, they did not have any means of locomotion and thus relied in full on the currents to deliver the nutrients required to replicate. As organisms evolved, they began to assert an increasing amount of control over their environment. A primitive example can be found in chemotaxis, where ciliae and flagellae, coupled with simple biochemical sensors allow the organism to follow biochemical gradients and thus actively gather nutrients.

Because flagellar motion is limited to either tumbles – random reorientation movements – or runs, the pathway linking these flagella to the biochemical sensors consists of only a few steps. With the advent of more advanced sensory and motoric systems, both the amount of information available to drive behaviour as well as the amount of possible behaviours available increased tremendously. As direct biochemical links between perception and action were no longer sufficient, simple neural network evolved to process the sensory signals allowing for abstract decisions about movement. As organisms became more complex, these networks evolved into a complete (central) nervous system, culminating in the cerebral cortex.

In most higher organisms, including humans, the amount of data collected by the sensory organs is enormous, the visual system alone generating between $10^7$ and $10^9$ bits per second \cite{koch2006,kelly1962}. The nervous system, and the cerebral cortex in particular, seem to use probabilistic models to compress the vast amount of the incoming sensory data \cite{zhaoping2006}. The principal idea behind these models being that they reduce redundancy in the input by knowledge about the statistics of the natural world.

Predictive models, or maps, on the spatial organization of the environment are of particular interest to an organism, as they provide for example probable locations of food and predators. To use these maps, the brain needs to know both the location and orientation of the body within the environment. In this thesis, we will investigate how the brain processes the available sensory signals in the perception of gravity as well as in the internal estimation of self-motion. The goal is to build computational models and perform thorough psychometric testing in order to examine the constraints that physics and biology impose on the interaction between the vestibular and other sensory systems. 

In the following section, sensory sources for gravity and self-motion perception are described in more detail. We then elaborate on how these signals may be used and integrated in the brain, and conclude the chapter with methods for studying the processing of these signals. 
 


\section{Sensory signals for navigation}

The signals used for self-motion and orientation perception, can be split in two broad categories: absolute signals and relative signals. Absolute signals, such as landmarks, can be used to directly estimate the location and orientation of an animal while relative signals, such as acceleration, first need to be (mathematically) integrated (i.e. dead reckoning or path integration) and then used to update previous estimates of position and orientation. While many sensory organs supply information used to estimate position and orientation, there is one sensory system that evolved specifically for this purpose: the vestibular system (see \figref{intro:fig6}). Its two components, the semicircular canals and the otoliths, are housed in the labyrinth of the temporal bone in the inner ear and are sensitive to angular velocity and linear acceleration, respectively.


\subsection{Vestibular system}

\begin{figure}
    \includegraphics[width=0.5\textwidth]{src/intro/figures/vestibular.pdf}
    \caption{Schematic view of the inner ear complex consisting of the auditory system (cochlea) and the vestibular system. The vestibular system has three angular velocity (semicircular canals) and two linear acceleration (saccule and utricle) sensors.}
    \label{intro:fig6}
\end{figure}


\subsubsection{Semicircular canals}

The semicircular canals measure the three-dimensional rotational velocity of the head. Each side of the head contains three orthogonally oriented canals allowing for rotation to be perceived in all three dimensions. Each of the six canals consists of a circular tube filled with a fluid known as endolymph (see \figref{intro:fig3}A). One part of the tube, the ampulla, is a bit thicker than the rest and contains a membrane, the cupula, that separates the fluid. When the head rotates, the fluid stays behind because of its inertia which in turn causes the membrane to deflect (see \figref{intro:fig3}B). While the inertial fluid motion suggests that the cupula should be sensitive to angular acceleration, reactive forces resulting from the fluid motion, such as endolymph viscosity and cupular elasticity,  cause the cupular deflection to reflect angular velocity instead \cite{goldberg2012}.

\begin{figure}
    \includegraphics[width=1.0\textwidth]{src/intro/figures/canals.pdf}
    \caption{Detailed view of a semicircular canal \panelref{A} in rest and \panelref{B} during rotation. Inertia of the endolymph in the canal causes the cupula to deflect during rotation.}
    \label{intro:fig3}
\end{figure}

The deflection of the cupula is transduced by special hair cells which are partially embedded in the cupula. These hair cells contain bundles of small protrusions, or hairs, on their apical surface. Each bundle contains several smaller stereocilia mechanically linked to one larger kinocilium \cite{pickles1984}. When the stereocilia are stretched towards the kinocilium the links cause cation channels (mechanoelectric transducers or METs) to open and the membrane to depolarize. Glutamate is then released into the synapse and causes the afferent neuron to depolarize and, after sufficient depolarizations, to generate action potentials \cite{purves2012}. These action potentials transmit the angular velocity sensed by the canals to the central nervous system through the  vestibulocochlear nerve.

Initially, the afferent signal closely follows rotational velocity, as opposed to acceleration. During sustained rotation, the elasticity of the cupula returns it to its resting position with causes the rotational signal to subside slowly. This is partly compensated for through velocity storage in the central nervous system. The canals are only sensitive to changes in orientation, as their circular nature makes them insensitive to the effects of linear acceleration and gravity \cite{goldberg2012}. 


\subsubsection{Otoliths}
The linear acceleration and gravitational forces are measured by the two otolith organs on either side of the head: the saccule and the utricule. Each otolith consists of an endolymph filled compartment containing calcium carbonate crystals known as otoconia. Due to their high inertia, these otoconia fall behind during linear acceleration, and move ahead during deceleration. In addition, they fall downward as a result of the gravitational field. The otoconia are mounted on top of a flexible polysaccharide gel, in which hair cells, similar to those found in the semicircular canals, are partly embedded \cite{goldberg2012}.  

Einstein's equivalence principle states that  it is not possible to measure forces caused by linear acceleration and by gravity independently. The signal coming from the otoliths is therefore proportional to the combination of these forces \cite{fernandez1976b}, which is commonly referred to as the gravito-inertial force (GIF). Compared to the three semicircular canals, which are able to sense rotation in three dimensions due to their orthogonal organization, either side of the head only contains two otolith organs. It is still possible to sense the gravitoinertial force in three dimensions because the otolith organs are curved, and the orientation of the hair cells determined the direction of sensitivity \cite{goldberg2012}.

\begin{figure}
    \includegraphics[width=1.0\textwidth]{src/intro/figures/otoliths.pdf}
    \caption{Detailed view of an otolith \panelref{A} in rest, \panelref{B} during head tilt, and \panelref{C} during linear acceleration. The high inertia of the otoconia keeps them from moving rapidly during both head tilt as well as linear acceleration. As the effect is physically identical in either case, head tilt and linear acceleration cannot be disambiguated based on the otolith signal alone.}
    \label{intro:fig7}
\end{figure}

For many actions, the brain needs to disentangle the contributions of gravity and linear acceleration to the gravitointertial force. For example, the linear vestibuloocular reflex (LVOR) that stabilizes gaze during translation should be sensitive to translation while ignoring gravity. Under normal circumstances, the brain is able to perform this task well \cite{merfeld1995}, but during extreme conditions such as in airplanes or space flight errors might occur. The somatogravic illusion \cite{glasauer1995} is an example of such error. In this illusion an airplane accelerates forward which causes an inertial force in the opposite direction. The direction of the resulting GIF will therefore be in between gravity (downward) and the intertial force (backward). The brain erroneously interprets the (majority of the) GIF as being caused by gravity, leading to the perception of "nose-up" pitch tilt.

Various theories have been proposed as to how the brain might solve the tilt-translation ambiguity. The frequency segregation hypothesis, for example, makes use of the fact that when stationary, the only force we experience is gravity. In this case, sustained (low frequency) accelerations should be  attributed to gravity, while the high frequency components should be attributed to translation \cite{paige1991, telford1997}. Other models keep track of the expected direction of gravity by integrating over the vector product of gravity and angular velocity from the semi-circular canals, and subtracting that signal from the otolith signal to obtain an estimate of linear acceleration. Because the semi-circular canals play a  crucial role in these models, they are known as multisensory integration models \cite{mayne1974,ormsby1977}. Merfeld and Zupan \citeyear{merfeld1995,merfeld2002} further refined the multisensory integration model by explicitly stating that the brain uses an (internal) model of the physical world to resolve the tilt-translation ambiguity.

The disambiguation of linear acceleration and gravity is not solely based on vestibular inputs, but also takes the  range of  possible movements into account. For example when moved on a linear sled, the probability of a participant perceiving tilt is greatly reduced \cite{wertheim2001}, indicating that cognitive processes also influence disambiguation.


\subsection{Somatosensory}
The gravito-inertial force (GIF) caused by a combination of gravity and linear acceleration is not only detected by the vestibular system, but also sensed by other sensory systems. Early evidence that non-vestibular sources were used by the brain came from DeKleyn and Versteegh \citeyear{dekleyn1933} who showed that inertial reflexes still occurred after removal of the otolith organs.

Since then, the contribution of specific organs to GIF perception has been demonstrated. In 1992, \citeauthor{mittelstaedt1992} rotated supine participants along  their naso-occipital axis,  causing  the centrifugal force to act on somatic GIF sensors  but not on the otoliths. In nephrectomized participants, the perceived direction of gravity relied less on the centrifugal force compared to controls, suggesting that the kidneys play a crucial role in perception of the gravitointertial force. Further evidence came from Trousselard \citeyear{trousselard2004}, who showed that the perception of gravity in a tilted position depends on whether the stomach is full or empty. In addition, reducing somatosensory cues, by applying a body cast, also affects the perception of gravity \cite{trousselard2004}.  Similar results have been obtained for many other visceral factors, such as the blood vessels \cite{vaitl2002}, and spinal axis fluid \cite{vaitl1997}.


\subsection{Visual}
Even though the vestibular and somatosensory systems directly measure the gravito-inertial force, orientation and navigational information can also be extracted from the visual system. In many cases, especially when the low latency of the vestibular signal is not required, the visual signal even overshadows the vestibular one \cite{wright2005,gaerlan2012}.

\subsubsection{Vection}
When we move through the environment, the image of the world on our retina shifts. This large-field shift pattern, also known as optic flow, depends on the movement being made. Lateral translation (\figref{intro:fig2}A) for example causes a different pattern than roll rotation (\figref{intro:fig2}B). At the turn of the 19th century von Helmholtz \citeyear{vonhelmholtz1867} recognized the importance of these flow signals for self-motion perception. In some cases the optic flow signal is so strong that it causes a percept of self-motion in stationary participants, called vection \cite{dichgans1978}. This is experienced, for example, when sitting on the train and a neighbouring train starts to move. This effect is much less likely to occur when e.g. on the platform, suggesting that the vection signal is integrated with prior knowledge about the environment before causing self-motion perception \cite{andersen1985,lepecq1995}


\begin{figure}
    \includegraphics[width=1.0\textwidth]{src/intro/figures/Vection.png}

    \caption{Vection pattern during \panelref{A} lateral translation and \panelref{B} rotation.}
    \label{intro:fig2}
\end{figure}

Similar retinal shifts are also observed during movement of the eyes or head. When interpreting optic flow the brain needs to distinguish object- from self-motion, this process is called optic flow parsing. One strategy is to use extra-retinal cues such as the vestibular, and somatosensory cues to self-motion described above, to subject out the retinal stimulation due to self-motion \cite{wertheim1994,wexler2001,macneilage2012}. Note that while this does accomplish the goal of disambiguation object- from self-motion, no information about self-motion is gained.

While the extra-retinal information does contribute to optic flow parsing, the existence of vection \cite{dichgans1978} suggests that the brain can parse optic flow using a purely visual approach \cite{rushton2005,warren2007}. Warren and Rushton \citeyear{warren2009} have shown that the brain indeed uses the global pattern of retinal motion caused by self-motion to parse optic flow. Even though global retinal flow patterns are used to disambiguate between object- and self-motion, it does not mean that a percept of self-motion, i.e. vection, exists. Research has shown that vection can take up to 30s to establish, but that purely visual optic flow parsing occurs within 1s \cite{warren2009}.

\subsubsection{Landmarks}
In addition to relative cues, the brain also seems to use absolute cues in both the perception of gravity and self-motion. For the perception of gravity, it makes use of the fact that many lines within the world are aligned with either the horizon or gravity. Straight lines therefore acts as a priors attracting the perceived direction of gravity towards them. A special case is the rod-and-frame illusion (\figref{intro:fig9}), where the perceived angle of a rod relative to gravity is affected by the orientation of the frame that contains it \cite{witkin1948}. 

\begin{figure}
	\includegraphics[width=1.0\textwidth]{src/intro/figures/treeframe.pdf}
	\caption{\panelref{A} As many edges in the world, most trees are aligned with gravity and can serve as absolute orientation cues.  \panelref{B} The rod-and-frame illusion.}
	\label{intro:fig9}
\end{figure}

Similarly, our position within the world can be established using world-fixed landmarks. With the exception of animals and vehicles, most items in the world rarely change position. By comparing our current visual scene (e.g. the Sequoias in \figref{intro:fig1}) with our knowledge about the environment we can determine our location (in this case northern California). Because the experiments outlined in this thesis were performed in near darkness, these absolute navigational cues can be considered less relevant. 


\subsubsection{Oculomotor}
Eye movements typically occur during self-motion to improve dynamic visual acuity and reduce retinal slip. In Chapter 3 of this thesis, we explore whether these extra-ocular eye movement have a reversed role, providing cues about self-motion perception beyond optic flow parsing. Previous research has hinted at an influence of eye movements on self-motion perception.

In 1963, \citeauthor{guedry1963} reported a substantial underestimation of displacement when their observers watched a small body-fixed target compared to displacements in the dark. Because of the VOR, eye movement in darkness are larger than those in the body-fixed condition. The underestimation in the body-fixed condition can therefore be interpreted as the involvement of eye movements, although the authors did not suggest this 
Studies on postural sway also indicate a role of eye movement is self-motion perception. Making eye movements causes postural sway to increase, suggesting that eye movements influence self-motion perception \cite{glasauer2005,rogrigues2015}.

One complication is that the magnitude, $e$, of these eye movements not only depend on translation amplitude, $T$, but also on fixation depth, d. When fixating position $(x, d)$, the eye position at starting position is $e_0 = atan⁡(d/x) \approx d/x$. By subtracting the eye position after the translation, $e_1 = atan⁡(d/(x+T)) \approx d/(x+T)$, we obtain the eye movement amplitude, $e \approx d/T$. In Chapter 4 we explore whether the brain takes this geometry into account and compensates for fixation depth when using eye movements in self-motion perception.


\section{Reflexive uses for the self-motion}
While vection is a good cue to self-motion perception, it can cause the image on the fovea to blur which hampers visual perception. Several reflexive mechanisms attempt to keep fixation on target during self-motion. Two examples are the vestibulo-ocular reflex, the VOR, where the low-latency vestibular signal drives the reflex and the optokinetic reflex, the OKR, where optic flow acts as the source signal.

The vestibulo-ocular reflex (VOR) compensates for both rotation as well as translation, it therefore consists of two reflex arcs. The translational VOR, or TVOR, is driven by the otolith signal while the rotational VOR, or RVOR, is mainly driven by the semicircular canals. The RVOR operates during head rotation by counter rotating the eyes in the opposite direction to the head. Because its gain is about one, eye velocity is about equal to the head velocity. The TVOR operates during head translation, when the head moves orthogonally to the line of sight, simple geometry requires that the ideal TVOR response is inversely proportional to fixation distance, $e_vor = atan⁡(d/T)$. As a result, no compensatory eye movements are required when fixating targets at infinity to maintain a stable retinal image. Because the latency of the vestibular signal is much lower compared to that of the OKR, it dominates at higher frequencies \cite{schweigard1997}.

The optokinetic reflex uses the optic-flow based signal to self-motion, that is vection, to compensate for both translational and rotational movement. Because the retinal cells driving the OKR are more sensitive to slow motion, the OKR itself compensates for low frequency movements. In contrast to the vestibularly driven VOR, it is even sensitive to constant velocity stimuli \cite{soodak1988}.

In both the OKR and the VOR the eye movements made consist of two parts, the slow-phase pursuit-like movement in which the eyes are kept on target using slow eye movements and the quick-phase saccades in which the eyes are quickly moved back to their central position. The fast movements are elicited when the eyes are about to lose track of the target because of physical constraints within the oculomotor system.



\section{Integration of signals for action and perception}

While reflexes by definition depend on minimally processed sensory signals, higher animal functions can rely on more elaborate processing. One such processing step is determining the underlying physical cause of a sensory signal. It has become clear that multiple sensory systems provide information about the same physical quantity. For example, both the visual and the vestibular system contribute to self-motion perception. A very naive approach to deal with these inputs would be to focus on the most reliable cue and ignore the others. A better solution, however, is to weight all available cues by their  relative reliabilities. This approach is known as statistically optimal, or Bayesian, integration, the latter referring to the use  of Bayes' inference. 

We will now briefly explain optimal integration from a mathematical perspective.  In this example there is a physical stimulus, $x$, which is observed by multiple sensory systems. We assume that each system adds independent Gaussian noise, $\sigma_i^2$, to its observation. The probability of the observations, $x_i$, given the stimulus $x$, is therefore:

\begin{equation}
P(x_i|x)= \mathcal{N}(x_i, \sigma_i^2)
\end{equation}

From the point of view of the brain, the outcomes, $x_i$, are given while the stimulus, $x$, has to be inferred. In this case, probability $P(x_i│x)$ is referred to as the likelihood of  the  stimulus given the observations, or $\mathcal{L}(x|x_i)$. Given multiple sensory observations, $x_1,...,x_n$, we can now compute the likelihood of the stimulus:

\begin{equation}
\mathcal{L}(x|x_1,...,x_n) = P(x_1,...,x_n|x) = \prod_i P(x_i|x)
\end{equation}

Making use of Bayes' rule, the  brain can infer the probability of the stimulus given its observations:

\begin{equation}
P(x|x_1,...,x_n) = \frac{P(x_1,...,x_n)P(x)}{P(x_1,...,x_n)}
\end{equation}

In this equation, $P(x)$ represents the prior probability of the stimulus, that is the probability of the stimulus occurring without taking the sensory input into account. The left-hand side of the equation, $P(x│x_1,..,x_n$, is commonly referred to as the posterior probability.

The  most likely stimulus given the observations is  the one for which the posterior probability is largest. This method of finding the most optimal estimate (\eqnref{intro:eq:map}) is known maximum a posteriori estimation (MAP).

\begin{equation}
\hat{x} = argmax_x P(x_1,...,x_n|x)P(x)
\label{intro:eq:map}
\end{equation}

When using a flat prior, that is when all stimuli are equally likely to occur, $P(x)=c$, \eqnref{intro:eq:map} can be simplified to

\begin{equation}
\hat{x} = argmax_x P(x_1,...,x_n|x)
\label{intro:eq:mle}
\end{equation}

In this case, we refer to it as maximum  likelihood estimation (MLE). Assuming a flat prior, the solution for the  most likely stimulus, $\hat{x}$, is a weighted sum of the sensory inputs,

\begin{equation}
\hat{x}=\sum_i w_i x_i
\end{equation}

with weight, $w_i$,

\begin{equation}
w_i = \frac{1/\sigma^2_i}{\sum_j 1/\sigma^2_j}
\end{equation}

As the posterior is a distribution, it also provides us with an estimate of uncertainty in  the most likely stimulus value,

\begin{equation}
\hat{\sigma}^2_i = \frac{\prod_i \sigma^2_i}{\sum_i \sigma^2_i}
\end{equation}

%[Here I want to briefly discuss some articles that show that optimal integration indeed takes place, perhaps write a note about cases in which fusion/integration should / should not occur]

%While reflexes depend on minimally processed sensory signals by definition, higher animal functions can rely on more elaborate processing. One such step is multi-sensory integration, combining multiple sensory signals into one abstract percept. Before such integration can occur, the brain first establishes that the available cues are in accordance with each other. If that is the case, fusion of the cues occurs, if not only one signal dominates self-motion perception while the other cues are either ignored or interpreted as object motion.

%In conflicting situations where visual cues contradict vestibular and proprioceptive ones a dominance of vision is observed \cite{berthoz1975}.

%In contrast, linear acceleration can change the perceived velocity of a moving visual scene, indicaing dominance of the vestibular over the visual signal \cite{pavard1977}. Mainly at low image velocities! At high image velocities the visual signal still dominates.

%As we have seen, multiple sensory systems provide information about both position and orientation at the same time. While the brain could take estimates from the most reliable system and ignore the rest, this is not what commonly happens.
%Instead, the brain tends to integrate all available evidence in what is called statistically optimal integration. In some cases prior knowledge, for example the fact that we stand mostly upright, is also taken into account. When this is the case, we talk about bayesian integration.


\section{Reference frames and spatial updating}
The output of a sensory system is relative to a specific frame of reference. For example, retinal information initially enters the brain in a gaze-fixed frame of reference while information from the vestibular enters in a head-fixed reference frame. When information from different sensory sources is combined, the brain needs to convert the information into a common frame of reference using what is called a reference frame transformation.

These reference frame transformations are not static, but depend on the position of our body. For example when converting the gaze-fixed retinal signal into a head-centred frame of reference, the position of the eyes have to be taken into account. When the position of our body changes, e.g. when we move our eyes, all coordinates remembered within the moving frame of reference also need to be updated. For example, when moving the eyes remembered retinal information needs to be shifted in the opposite direction to remain at the correct relative position. This process is known as spatial updating or spatial constancy.

\section{Studying sensory signals}
A large part of  our knowledge on these and other aspects of sensory processing comes from psychophysical experiments. In this type of experiments the relation between physical stimuli, e.g. rotation angle, and perception, e.g. perceived body orientation, is quantified.

The two-alternative forced choice, or 2AFC, paradigm is commonly used in psychophyiscal experiments. Participants are presented with two stimuli, e.g. two translation distances, and are then forced to make a choice, e.g. on which of the translations was longer. By structurally manipulating one of the choices the point of subjective equality, or PSE, can be determined, that is the point at which the participant is unsure, i.e. picks a response at random, and perceives the stimuli as being equal. The difference between the PSE and the actual stimulus is known as the bias, represented by the Greek letter mu ($\mu$).

In addition to establishing perceptual equality, the uncertainty on the responses can also be determined by looking at the response-distribution as a function of the manipulated variable. This uncertainty is commonly quantified by the standard deviation, represented by the Greek letter sigma ($\sigma$).

The next four sections will explain the four 2AFC tasks used in the present work in more detail.

\subsection{Subjective body tilt}
In the subjective body tilt, or SBT, task the perceived body orientation with respect to a given body tilt angle is probed. Participants are first given a reference angle, e.g. 45 \si{\degree}, and are then rotated to an angle close to the reference angle, e.g. 46 \si{\degree} (see \figref{intro:fig4}A). They then have to indicate whether their current orientation is clockwise or counter-clockwise with respect to the reference angle. By systematically probing rotation angles around the reference angle we can obtain both the bias and uncertainty on the percept of body tilt.

Most participants are able to do this perfectly, regardless of the reference angle (see \figref{intro:fig4}B). Their uncertainty does increase as a function of reference angle though (see \figref{intro:fig4}C).
As the SBT probes body orientation, the somatosensory signal originating from the torso can be used without any reference transformation and thus provides a direct contribution. Other sensory signals such as the vestibular signal can also be used, but only after a reference frame transformation.

\begin{figure}
    \includegraphics[width=1.0\textwidth]{src/intro/figures/SBT.png}

    \caption{}
    \label{intro:fig4}
\end{figure}


\subsection{Subjective visual vertical}
The subjective visual vertical, or SVV, task is a similar task in which participants have to judge the orientation of a line with respect to gravity. The PSE, that is the angle at which the line is perceived to be aligned with gravity, can be found by presenting lines at different angles and asking the participant whether the line is rotated clockwise or counter-clockwise relative to gravity (see \figref{intro:fig5}A).

When seated straight this task, participants do not make any static errors and are very certain about their responses (see \figref{intro:fig5}B and C). This changes when first rotating the participant before the task. In general, the static error increases with tilt angle, the direction of the error is in the direction of the body mid-line, this effect is known as the Aubert or A-effect. At smaller angles overcompensation occurs and the static error moves in the opposite direction, that is away from the body mid-line. This latter effect is known as the E-effect and could be due to the effects of ocular counter-roll (OCR).

\begin{figure}
    \includegraphics[width=1.0\textwidth]{src/intro/figures/SVV.png}

    \caption{}
    \label{intro:fig5}
\end{figure}


\subsection{?}

Perhaps explain the lateral translation task.

\subsection{Errors in spatial updating}

The errors in spatial updating tell us something about ... [expand on this].


\section{Outline of this thesis}
Multiple sensory signals provide information about both body orientation and self-motion. This thesis explores the how the brain integrates these signals to form dynamic but coherent percepts.

We start with Chapter 2 in which we investigate whether the visual, somatosensory, and vestibular cues to body orientation are integrated in a statistically optimal fashion. Because the contributions of these signals cannot be studied in isolation, we adopted an inverse approach where we assumed statistical optimality and attempted to compute the statistical properties of individual sensors. To this end, we fitted an optimal integration model to two physophyisical tasks, one gauging head-in-space orientation (SVV) and the other body-in-space orientation (SBT). Predictions based on these noise properties were consistent with previously published deficits of various patient groups, further strengthening the idea that human spatial orientation is statistically optimal.

In Chapter 3 we move on to the dynamic aspects of vestibular integration, investigating whether eye movements contribute to the perception of whole-body translation. Using a psychophysical task, we first shows that translations in which the eyes fixate a body-fixed target are perceived as shorter than those in which the eyes fixate make pursuit movements by fixating a world-fixed target. We then go on an show that this result does not depend on the presence of the fixation point by including an unconstrained eye movement condition. Because fixation distance influences the magnitude of pursuit eye movements, we fixed the distance of the fixation point in Chapter 3. To investigate whether the brain takes fixation distance into account, we compared self-motion perception during near and far world-fixed targets in Chapter 4. Our results suggest that translation is not properly accounted for, indicating that raw eye movements augment self-motion perception.

We conclude this thesis with Chapter 5, where we investigate how these self-motion signals are used to track previously seen objects across translation. First, we show that the errors made during spatial updating do not only depend on the location of the previously seen object, but also on the location of gaze. This is consistent with spatial updating in a gaze-centred reference frame. We further show that the underestimation of self-motion is a possible cause of the errors found.
