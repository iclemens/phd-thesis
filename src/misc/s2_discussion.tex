\chapter{Summary and discussion}

%%%%%%%%%%
% Introduction

\section{Summary}

%%%%%%%%%%%%%
% The idea is to concatenate the abstracts for all papers
% and then smoothen the text out a bit.
% Look at what Frank and Verena did in their Theses.

% 1st paper in thesis
Most evidence that the brain uses Bayesian inference to integrate noisy sensory signals optimally has been obtained by showing that the noise levels in each modality separately can predict performance in combined conditions. Such a forward approach is difficult to implement when the various signals cannot be measured in isolation, as in spatial orientation, which involves the processing of visual, somatosensory, and vestibular cues. Instead, we applied an inverse probabilistic approach, based on optimal observer theory. Our goal was to investigate whether the perceptual differences found when probing two different states ---body-in-space and head-in-space orientation--- can be reconciled by a shared scheme using all available sensory signals. Using a psychometric approach, seven human subjects were tested on two orientation estimates at tilts \textless 120\textdegree: perception of body tilt [subjective body tilt (SBT)] and perception of visual vertical [subjective visual vertical (SVV)]. In all subjects, the SBT was more accurate than the SVV, which showed substantial systematic errors for tilt angles beyond 60\textdegree. Variability increased with tilt angle in both tasks, but was consistently lower in the SVV. The sensory integration model fitted both datasets very nicely. A further experiment, in which supine subjects judged their head orientation relative to the body, independently confirmed the predicted head-on-body noise by the model. Model predictions based on the derived noise properties from the various modalities were also consistent with previously published deficits in vestibular and somatosensory patients. We conclude that Bayesian computations can account for the typical differences in spatial orientation judgments associated with different task requirements.

% 2nd paper in thesis
Self-motion is typically accompanied by compensatory eye movements that help minimize retinal slip and maximize dynamic visual acuity. To date, it is unknown whether these eye movements also have a reversed role, serving as a cue for self-motion perception. To address this question, we had participants ($n=8$) judge self-motion during different eye movement conditions in the absence of full-field optic flow.  In a 2-AFC task, participants indicated whether the second of two successive, passive, lateral whole-body translations was longer or shorter than the first. Eye movements during each translation were world-stationary, body-stationary or free in an otherwise dark room. Results show that the perceived translations were shorter with body-fixed gaze compared to world-fixed gaze. Using a linear model, we estimated the vestibular and eye movement contributions to self-motion perception and found that eye movement signals contribute approximately 25 percent. The model was validated independently by predicting the effects of eye movements on self-motion perception during the free eye movement conditions, taking the inter-subject variation in oculomotor weight and eye movement patterns into account. We conclude that eye movement signals influence self-motion perception, even in the absence of visual stimulation, and even when oculomotor and vestibular estimates are in conflict, e.g. during body-fixed gaze. We hypothesize that adverse consequences of this seemingly inflexible arrangement are minimal under natural conditions because eye movements and self-motion are highly correlated, and because eye movements are most often accompanied by veridical optic flow cues to self-motion.

% 3rd paper in thesis (see paper 4)

% 4th paper in thesis
In order to maintain visual stability during self-motion, the brain needs to update any ego-centric spatial representations of the environment. Here, we use a novel psychophysical approach to investigate how, and to which extent, the brain integrates visual, extraocular, and vestibular signals pertaining to this spatial update. Participants were oscillated sideways at a frequency of 0.63 Hz while keeping gaze fixed on a stationary light. When the motion direction changed, a reference target was shown either in front or behind the fixation point. At the next reversal, half a cycle later, we tested updating of this reference location by asking participants to judge whether a briefly flashed probe was shown to the left or right of the memorized target. We show that updating is not only biased, but that the direction and magnitude of this bias depend on both gaze and object location, implying that a gaze-centered reference frame is involved. Using geometric modeling, we further show that the gaze-dependent errors can be caused by an underestimation of translation amplitude, by a bias of visually perceived objects towards the fovea (i.e., a foveal bias), or by a combination of both.

\section{Discussion}

%[Could it be that the fixation target is a world-fixed reference? Add something about that here or later.]

%%%%%%%%%%%%%
% Integrated model

Could check Anna/Alexandra results again -> updating + self-motion effect.
Reaching poster I've made for SfN

%%%%%%%%%%%%%%
% Relation to other studies

Based on our updating and self-motion perception studies -> is it possible to make predictions for other studies?
What would we predict van Pelt would do for instance?

%%%%%%%%%%%%%%
% Conclusion

