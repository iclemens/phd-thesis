
\chapter{Summary and discussion}
\chaptermark{}
\newpage

Navigating through the environment evokes complex changes of visual, auditory, vestibular, tactile and
motor inputs to the brain. Yet, despite these motion induced changes of input, we perceive the world as a stable reality, maintain an integrated sense of where we are, how we are oriented, and are able to track and act rather effortlessly upon surrounding objects. How this ability comes about is also the topic of this thesis. The objective of the research project described in this thesis was to build computational models and perform thorough psychometric testing in order to unravel the physical and biological constraints on the interaction between the vestibular and other sensory systems for spatial orientation and self-motion perception. This resulted in the following contributions to the field:

\begin{description}
\item[\Chapter{p1}] Statistical optimality can account for the way body somatosensory, neck proprioceptive and vestibular signals are integrated in spatial orientation perception. While these sensors cannot be examined in isolation, by using optimality as a starting point, their noise properties can be determined and linked to clinical deficits that are seen in particular patient groups.
\item[\Chapter{p3}] Oculomotor signals influence self-motion perception, even in the absence of optic flow or other visual stimulation, and even when in conflict with vestibular information. 
\item[\Chapter{p4}] This oculomotor signals should be regarded as a rudimentary cue to self-motion perception; it is not veridically scaled by fixation distance in the perception of body translation. 
\item[\Chapter{p2}] Self-motion signals interact with the dynamic perception of external world.  Errors that arise in this process suggest the use of a gaze-centred reference frames in the underlying computations. 
\end{description}

In the following sections, we will provide a detailed summary of each result. 

\section{Multisensory processing in spatial orientation}
Many studies have shown that the brain combines noisy sensory signals in a statistically optimal way. This is normally done by showing that the noise levels of each separate modality can predict performance in combined conditions. In spatial orientation it is difficult to measure sensory signals in isolation as one cannot switch off the vestibular sense when measuring somatosensory contributions. In \Chapter{p1} we therefore used a statistically optimal integration model as a starting point, and attempted to account for the perceptual differences found when probing head-in-space versus body-in-space orientation. Using a psychometric approach, we tested both the perception of body tilt (subjective body tilt; SBT) as well as the perception of visual vertical (subjective visual vertical; SVV) in seven participants. Because both the SVV and SBT make use of the same sensory signals, we were able to fit a 7-parameter probabilistic model to the response data.  One  of the estimated parameters represented the noise of the neck proprioceptors. This allowed us to independently confirm that the derived values for neck noise matched those that were measured in isolation. We further validated our model by showing that predictions made by our model are consistent with previously published deficits in vestibular and somatosensory patients. We conclude that Bayesian computations can account for the typical differences in spatial orientation judgements associated with different task requirements. In a follow-up to this work, this approach was recently applied to a patient population with complete  vestibular loss \cite{alberts2015}. Performance in those patients was similar to that of controls, suggesting that the sensory weights had shifted from the vestibular to the somatic sensors.

\section{Eye movements influence self-motion perception}
Eye movement typically accompany self-motion in order to minimise retinal slip and maximise dynamic visual acuity. In \Chapter{p3} we investigated whether these eye movements also have a reversed role, by serving as a cue for self-motion perception. To address this question, we asked participants to compare perceived translation distances from two successive, passive, lateral whole-body translations. Eye movements during these translations were either world-stationary or body-stationary. Results show that translations were perceived shorter with body-fixed gaze compared to world-fixed gaze, indicating that eye movements indeed influence self-motion perception. Using a linear model, we estimated the relative contribution of the vestibular versus the eye movement based displacement signal: the eye movement based displacement signal  contributes approximately 25 percent to the perceived motion. We independently validated the model by successfully predicting the effects of eye movements on self-motion perception during trials in which the eye movements were unconstrained. This further shows that eye movement signals influence self-motion perception, even in the absence of visual stimulation, and even when oculomotor and vestibular estimates are in conflict, e.g. during body-fixed gaze. We hypothesise that adverse consequences of this seemingly inflexible arrangement are minimal under natural conditions because eye movements and self-motion are highly correlated, and because eye movements are most often accompanied by veridical optic flow cues to self-motion.

\section{Partial compensation for fixation depth in self-motion perception}
If eye movements are used in the estimation of self-motion magnitude, the brain should also take the accompanying fixation depth into account for a veridical translation estimate. The reason is that the amplitude of these eye movements, for the same physical translation, depends on the depth of fixation: when fixating faraway they are smaller than when fixating nearby in the world.  In \Chapter{p4} we investigated whether the brain indeed takes fixation depth into account when using eye movements to augment self-motion perception. Participants had to judge self-motion during different eye movement conditions in the absence of full-field optic flow. In a 2-AFC task, similar to the one used in \Chapter{p3}, participants indicated whether the second of two successive passive lateral whole-body translations was longer or shorter than the first. During each translation, participants fixated either a nearby or far away target, which was either body- or world-stationary. Results show that the perceived translations were shorter for nearby world-fixed gaze compared to faraway world-fixed gaze, indicating that eye movements are not properly scaled in self-motion perception. Together with the observation that self-motion perception is not affected by the depth of a body stationary fixation target, we conclude that eye movements are merely a rudimentary cue to self-motion, with a compensation for fixation depth that is partial at best.

\section{Gaze-dependent effects in spatial updating}
The brain also needs self-motion signals to update ego-centric spatial representations of the environment. In \Chapter{p2} we investigate how, and to which extent, the brain integrates the various self-motion signals for the spatial update. Participants were oscillated sideways while keeping gaze fixed on a stationary target. When the motion direction changed, a reference target was shown either in front or behind the fixation point. Half a cycle later, at the next reversal, we tested updating of this reference location by asking participants to judge whether a briefly flashed probe was shown to the left or right of the memorised target. Results show that both the direction and magnitude of the bias in spatial updating depends on the location of the object being updated with respect to gaze, implying that a gaze-centred reference frame is involved. We further show that these biases can be caused by an underestimated of translation amplitude, a bias of visually perceived objects towards the fovea, or by a combination of both.

\section{Concluding remarks}

All experimental chapters in this thesis report on subjects' spatial perception. In \Chapter{p1} we use an optimal statistical integration model of all the contributing sensory systems to explain both the bias and precision of these percepts. In these models the contributions of the senses are weighted according to their uncertainty. In the chapters on lateral displacement perception (\Chapter{p3,p4}) we also use weightings of the vestibular and eye-movement based estimates of displacement. However, these weights are only based on the observed biases in self-motion perception. Ideally one would use the same approach as in \Chapter{p1} to model and analyse these self-motion perception data. We have made initial efforts to arrive at such a model, but there appear at least two major challenges. First, the geometry of the self-motion perception experiments makes that the straightforward Gaussian distributions from the spatial orientation model become less well defined, skewed distributions and we would have use particle filter models to run our simulations. Second, those models contain many free parameters for the sensory modalities and priors. With the current set of experiments we do not have enough data to fit these parameters in a consistent manner. Future research should be done to derive a complete optimal integration account of self-motion perception.

In \Chapter{p2} we show that eye movements elicited by fixating a world-stationary target are taken into account when updating the remembered location of previously seen targets.  If the underlying signals used for self-motion perception as found in \Chapter{p3,p4} are also used for spatial updating, the type of fixation (world- versus body-stationary) should have an effect on the observed biases in spatial updating as well. Indeed, results from a pilot experiment in the early stage of this thesis suggest that fixation type influences spatial updating performance under translation \cite{clemens2010}. Participants had to reach towards remembered targets after an intervening translation. Preliminary results suggest that reaching errors are large when fixating a body-stationary target during translation, while no such errors exist while fixating world-stationary targets \cite{clemens2010}, suggesting that indeed the eye-movement based self-motion cues are also used in spatial updating.

In conclusion, I hope to have made new advances in the understanding of the mechanisms for spatial orientation and self-motion perception. Of course, there are many remaining questions for further study. These studies should not only address the computational and theoretical mechanisms but also on the neural implementation and pathways that can be found in the brain.

 