\chapter{Summary and discussion}

Navigating through the environment evokes complex changes of visual, auditory, vestibular, tactile and
motor inputs to the brain. Yet, despite this motion, we perceive the world as a stable reality, maintain an integrated sense of where we are, how we are oriented, and are able to track and act rather effortlessly upon surrounding objects.  How the ability comes about it one of the key questions in neuroscience and also studied in this research project. The objective was to build computational models and perform thorough psychometric testing in order to examine the physical and biological contrast that are placed on the interaction between the vestibular and other sensory systems for spatial orientation and self-motion perception. This resulted in the following contributions to the field:

\begin{enumerate}
\item Statistical optimality can account for the way body somatosensory, neck proprioceptive and vestibular signals are integrated in spatial orientation perception. While these sensors cannot be examined in isolation, by using optimality as a starting point, their noise properties can be determined and linked to clinical deficits that are seen in particular patient groups.
\item Oculomotor signals influence self-motion perception, even in the absence of optic flow or other visual stimulation, and even when in conflict with vestibular information. 
\item This oculomotor signals should be regarded as rudimentary cue to self-motion perception; it is not veridically scaled by fixation distance in the perception of body translation. 
\item Self-motion signals interact with the dynamic perception of external world.  Errors that arise in this process suggest the use of a gaze-centered reference frames in the underlying computations. 
\end{enumerate}

In the following sections, we will provide a detailed summary of each result. 

\section{Multisensory processing in spatial orientation}
Many studies have shown that the brain tends to combine noisy sensory signals in a statistically optimal way. This is normally done by showing that the noise levels of each separate modality can predict performance in combined conditions. In spatial orientation it is difficult to measure sensory signals in isolation as multisensory integration always takes place. In Chapter 2 we therefore used a statistically optimal integration model as a starting point, and attempted to account for the perceptual differences found when probing head-in-space versus body-in-space orientation. Using a psychometric approach, we tested both the perception of body tilt (subjective body tilt; SBT) as well as the perception of visual vertical (subjective visual vertical; SVV) in seven participants. Our model was able to explain the results from both tests. Because one of sensory modalities used, the noise in the neck proprioceptors, can be measured in relative isolation, we used it to independently confirm the predicted head-on-body noise found by the model. We further validated our model by showing that predictions made by our model are consistent with previously published deficits in vestibular and somatosensory patients. We conclude that Bayesian computations can account for the typical differences in spatial orientation judgments associated with different task requirements.

\section{Eye movements influence self-motion perception}
Eye movement typically accompany self-motion in order to minimize retinal slip and maximize dynamic visual acuity. In Chapter 3 we investigate whether these eye movements also have a reversed role, serving as a cue for self-motion perception. To address this question, we asked participants to compare perceived translation distance in two successive, passive, lateral whole-body translations. Eye movements during these translations were either world-stationary or body-stationary. Results show that translations were perceived shorter with body-fixed gaze compared to world-fixed gaze, indicating that eye movements indeed influence self-motion perception. Using a linear model, we estimated the relative contribution of the vestibular versus the eye movement signal, and found that eye movement signals contribute approximately 25 percent of the perceived motion. We independently validated the model by successfully predicting the effects of eye movements on self-motion perception during trails in which the eye movements were unconstrained. This further shows that eye movement signals influence self-motion perception, even in the absence of visual stimulation, and even when oculomotor and vestibular estimates are in conflict, e.g. during body-fixed gaze. We hypothesize that adverse consequences of this seemingly inflexible arrangement are minimal under natural conditions because eye movements and self-motion are highly correlated, and because eye movements are most often accompanied by veridical optic flow cues to self-motion.

\section{Partial compensation for fixation depth in self-motion perception}
The amplitude of these eye movements depend on the depth of fixation, when fixating  faraway they are smaller than when fixating nearby in the world. If these eye movements are used to augment self-motion perception, the effect of fixation depth has to be taken into account in order for self-motion perception to be veridical. In Chapter 4 we investigate whether the brain indeed compensates and takes fixation depth into account. Participants had to judge self-motion during different eye movement conditions in the absence of full-field optic flow. In a 2-AFC task, similar to the one used in Chapter 3, participants indicated whether the second of two successive passive lateral whole-body translations was longer or shorter than the first. During each translation, participants fixated either a nearby or far away target, which was either body- or world-stationary. Results show that the perceived translations were shorter for nearby world-fixed gaze compared to far away world-fixed gaze, indicating that eye movements are not properly scaled in self-motion perception. Together with the observation that self-motion perception is not affected by the depth of a body stationary fixation target, we conclude that eye movements are merely a rudimentary cue to self-motion, with a compensation for fixation depth that is partial at best.

\section{Gaze-dependent effects in spatial updating}
The brain uses these self-motion signals to update ego-centric spatial representations of the environment. In Chapter 5 we investigate how, and to which extent, the brain integrates these various self-motion signals for the spatial update. Participants were oscillated sideways while keeping gaze fixed on a stationary target. When the motion direction changed, a reference target was shown either in front or behind the fixation point. Half a cycle later, at the next reversal, we tested updating of this reference location by asking participants to judge whether a briefly flashed probe was shown to the left or right of the memorized target. Results show that both the direction and magnitude of the bias in spatial updating depends on the location of the object being updated with respect to gaze, implying that a gaze-centered reference frame is involved. We further show that these biases can be caused by an underestimated of translation amplitude, a bias of visually perceived objects towards the fovea, or by a combination of both.

\section{Concluding remarks}

To integrate multiple redundant sensory signals into a single percept, the brain commonly employs a process called Bayesian integration (see Chapter 1). In this process both sensory signals as well as prior information about the environment are combined by weighting them by their relative precision. Inversely, by assuming Bayesâ€™ optimal integration it is, for some systems, possbile to estimate the precision of sensory signals. In Chapter 2 we have demonstrated this concept for spatial orientation perception.

In Chapter 3 and 4 we have introduced eye movements as a novel cue to self-motion. Whether these  eye movements are integrated with out self-motion cues in a statiscially optimal fashion remains an open question, as our models did not take precision into account. While we hypothesize that this is indeed  the case, the weight of the oculomotor signal might not be easily determined.

Fixation targets can be world-stationary, body-stationary or somewhere in-between. Only when a fixation target is world-stationary should the eye movement signal be used for self-motion perception, otherwise it is uninformative. From a probablisitic perspective, one would weight the self-motion estimate given a world-stationary target (x=ed) and the estimate given a body-stationary target (x=0) based on the relative probability of the fixation target beging world- or body-stationary respectively. In darkness only prior knowledge can be used, and we hypothesize that the prior probability of fixating a world-stationary target is high. This in turn causes the eye movement signal to have a significant weight and thus contribution to the oculomotor based self-motion estimate. The reliability of the oculomotor signal can be weighted in a similar way, taking the probability of  world- versus body-stationary fixations into account.

In Chapter 5 we show that eye movements are taken into account when updating the remebered location of previously seen targets. If the same signal used for self-motion perception influences spatial updating, then the type of fixation (world- versus body-stationary) should have an effect on spatial updating as well. Furthermore any increase in noise due to the  updating process should be proportional to the differences in noise observed in the oculomotor-based self-motion estimates. 

We have performed a pilot experiment where participants had to reach towards remembered targets after intervening translation. Preliminary results suggest that reaching errors are large when fixating a  body-stationary target during translation, while no such errors exist while fixating world-stationary targets \cite{clemens2010}. Suggesting that the eye-movement based self-motion cues are indeed used in spatial updating.
 